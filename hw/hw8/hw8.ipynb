{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 8: Time series\n",
    "**Due date: [April 10, 11:59pm](https://github.com/UBC-CS/cpsc330-2023W2?tab=readme-ov-file#deliverable-due-dates-tentative).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "<hr>\n",
    "\n",
    "_points: 2_\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. To submit this assignment, follow the instructions below:\n",
    "\n",
    "- **You may work on this assignment in a group (group size <= 4) and submit your assignment as a group.** \n",
    "- Below are some instructions on working as a group.  \n",
    "    - The maximum group size is 4. \n",
    "    - You can choose your own group members. \n",
    "    - Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "    - Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "    - It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. [Here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members) are some instructions on adding group members in Gradescope.  \n",
    "- Be sure to follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W2/blob/main/docs/homework_instructions.md).\n",
    "- Upload the .ipynb file to PrairieLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: time series prediction\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset. We will be forcasting average avocado price for the next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1 2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2 2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3 2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4 2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~2 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '20170925'\n",
    "df_train = df[df[\"Date\"] <= split_date]\n",
    "df_test  = df[df[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many time series? \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "In the [Rain in Australia](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) dataset from lecture, we had different measurements for each Location. How about this avocado sales dataset? For which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15441 entries, 0 to 50\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Date          15441 non-null  datetime64[ns]\n",
      " 1   AveragePrice  15441 non-null  float64       \n",
      " 2   Total Volume  15441 non-null  float64       \n",
      " 3   4046          15441 non-null  float64       \n",
      " 4   4225          15441 non-null  float64       \n",
      " 5   4770          15441 non-null  float64       \n",
      " 6   Total Bags    15441 non-null  float64       \n",
      " 7   Small Bags    15441 non-null  float64       \n",
      " 8   Large Bags    15441 non-null  float64       \n",
      " 9   XLarge Bags   15441 non-null  float64       \n",
      " 10  type          15441 non-null  object        \n",
      " 11  year          15441 non-null  int64         \n",
      " 12  region        15441 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 categorical features in this dataset: \"type\" and \"region\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1373.95</td>\n",
       "      <td>57.42</td>\n",
       "      <td>153.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1182.56</td>\n",
       "      <td>39.00</td>\n",
       "      <td>305.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>838.44</td>\n",
       "      <td>838.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "51 2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "51 2015-01-04          1.79       1373.95    57.42    153.88    0.00   \n",
       "50 2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "50 2015-01-11          1.77       1182.56    39.00    305.12    0.00   \n",
       "49 2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51     9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "51     1162.65     1162.65        0.00          0.0       organic  2015   \n",
       "50     8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "50      838.44      838.44        0.00          0.0       organic  2015   \n",
       "49    11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "\n",
       "    region  \n",
       "51  Albany  \n",
       "51  Albany  \n",
       "50  Albany  \n",
       "50  Albany  \n",
       "49  Albany  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"region\", \"Date\"]).head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>435021.49</td>\n",
       "      <td>364302.39</td>\n",
       "      <td>23821.16</td>\n",
       "      <td>82.15</td>\n",
       "      <td>46815.79</td>\n",
       "      <td>16707.15</td>\n",
       "      <td>30108.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Atlanta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>788025.06</td>\n",
       "      <td>53987.31</td>\n",
       "      <td>552906.04</td>\n",
       "      <td>39995.03</td>\n",
       "      <td>141136.68</td>\n",
       "      <td>137146.07</td>\n",
       "      <td>3990.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>BaltimoreWashington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.01</td>\n",
       "      <td>80034.32</td>\n",
       "      <td>44562.12</td>\n",
       "      <td>24964.23</td>\n",
       "      <td>2752.35</td>\n",
       "      <td>7755.62</td>\n",
       "      <td>6064.30</td>\n",
       "      <td>1691.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Boise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.02</td>\n",
       "      <td>491738.00</td>\n",
       "      <td>7193.87</td>\n",
       "      <td>396752.18</td>\n",
       "      <td>128.82</td>\n",
       "      <td>87663.13</td>\n",
       "      <td>87406.84</td>\n",
       "      <td>256.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Boston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  Total Volume       4046       4225      4770  \\\n",
       "51 2015-01-04          1.22      40873.28    2819.50   28287.42     49.90   \n",
       "51 2015-01-04          1.00     435021.49  364302.39   23821.16     82.15   \n",
       "51 2015-01-04          1.08     788025.06   53987.31  552906.04  39995.03   \n",
       "51 2015-01-04          1.01      80034.32   44562.12   24964.23   2752.35   \n",
       "51 2015-01-04          1.02     491738.00    7193.87  396752.18    128.82   \n",
       "\n",
       "    Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "51     9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "51    46815.79    16707.15    30108.64          0.0  conventional  2015   \n",
       "51   141136.68   137146.07     3990.61          0.0  conventional  2015   \n",
       "51     7755.62     6064.30     1691.32          0.0  conventional  2015   \n",
       "51    87663.13    87406.84      256.29          0.0  conventional  2015   \n",
       "\n",
       "                 region  \n",
       "51               Albany  \n",
       "51              Atlanta  \n",
       "51  BaltimoreWashington  \n",
       "51                Boise  \n",
       "51               Boston  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=[\"type\", \"Date\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple measurements for both regions and type. When sorting the dataframe by date and region, we see that there are multiple measurements for the same region on the same day. We see that there are also multiple measurements for the same type on the same day when sorting by date and type instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Equally spaced measurements? \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Region          Type  7 days  14 days  21 days\n",
      "0                 Albany  conventional     168      NaN      NaN\n",
      "1                 Albany       organic     168      NaN      NaN\n",
      "2                Atlanta  conventional     168      NaN      NaN\n",
      "3                Atlanta       organic     168      NaN      NaN\n",
      "4    BaltimoreWashington  conventional     168      NaN      NaN\n",
      "..                   ...           ...     ...      ...      ...\n",
      "103              TotalUS       organic     168      NaN      NaN\n",
      "104                 West  conventional     168      NaN      NaN\n",
      "105                 West       organic     168      NaN      NaN\n",
      "106     WestTexNewMexico  conventional     168      NaN      NaN\n",
      "107     WestTexNewMexico       organic     163      1.0      1.0\n",
      "\n",
      "[108 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "result_data = []\n",
    "\n",
    "for (region, type_), group in df.groupby(['region', 'type']):\n",
    "    diff_counts = group[\"Date\"].sort_values().diff().value_counts()\n",
    "    row_data = {\"Region\": region, \"Type\": type_}\n",
    "    for index, value in diff_counts.items():\n",
    "        row_data[\"%d days\" % index.days] = value\n",
    "    result_data.append(row_data)\n",
    "\n",
    "result_df = pd.DataFrame(result_data)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results in this dataset are generally equally spaced with measurements at 7 days for all but 1 location. For the region \"WestTexNewMexico\" with type \"organic\", there are measurements at 7 days, 14 days, and 21 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Interpreting regions \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "In the Rain in Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are also all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albany', 'Atlanta', 'BaltimoreWashington', 'Boise', 'Boston',\n",
       "       'BuffaloRochester', 'California', 'Charlotte', 'Chicago',\n",
       "       'CincinnatiDayton', 'Columbus', 'DallasFtWorth', 'Denver',\n",
       "       'Detroit', 'GrandRapids', 'GreatLakes', 'HarrisburgScranton',\n",
       "       'HartfordSpringfield', 'Houston', 'Indianapolis', 'Jacksonville',\n",
       "       'LasVegas', 'LosAngeles', 'Louisville', 'MiamiFtLauderdale',\n",
       "       'Midsouth', 'Nashville', 'NewOrleansMobile', 'NewYork',\n",
       "       'Northeast', 'NorthernNewEngland', 'Orlando', 'Philadelphia',\n",
       "       'PhoenixTucson', 'Pittsburgh', 'Plains', 'Portland',\n",
       "       'RaleighGreensboro', 'RichmondNorfolk', 'Roanoke', 'Sacramento',\n",
       "       'SanDiego', 'SanFrancisco', 'Seattle', 'SouthCarolina',\n",
       "       'SouthCentral', 'Southeast', 'Spokane', 'StLouis', 'Syracuse',\n",
       "       'Tampa', 'TotalUS', 'West', 'WestTexNewMexico'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are overlapping regions in this dataset. For example, California is a distinct region in the dataset, but several other regions in the dataset are cities within California (including SanDiego, Sacramento, LosAngeles). There is also a \"West\" region that may refer to all of the western United States, and a \"TotalUS\" region which would include all other regions within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We will be trying to forecast the avocado price. The function below is adapted from [Lecture 19](https://github.com/UBC-CS/cpsc330-2023W2/blob/main/lectures/mathias/19_time-series.ipynb), with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag, groupby, new_feature_name=None, clip=False):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "    \n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "    \n",
    "    new_df = df.assign(**{new_feature_name : np.nan})\n",
    "    for name, group in new_df.groupby(groupby):        \n",
    "        if lag < 0: # take values from the past\n",
    "            new_df.loc[group.index[-lag:],new_feature_name] = group.iloc[:lag][orig_feature].values\n",
    "        else:       # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][orig_feature].values\n",
    "            \n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0 2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1 2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2 2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3 2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4 2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     9716.46     9186.93      529.53          0.0  conventional  2015  Albany  \n",
       "1     8424.77     8036.04      388.73          0.0  conventional  2015  Albany  \n",
       "2    11921.05    11651.09      269.96          0.0  conventional  2015  Albany  \n",
       "3    10845.82    10103.35      742.47          0.0  conventional  2015  Albany  \n",
       "4     9323.18     9170.82      152.36          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0 2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1 2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2 2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3 2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4 2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0     9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1     8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2    11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3    10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4     9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "\n",
       "   region  AveragePriceNextWeek  \n",
       "0  Albany                  1.24  \n",
       "1  Albany                  1.17  \n",
       "2  Albany                  1.06  \n",
       "3  Albany                  0.99  \n",
       "4  Albany                  0.99  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True)\n",
    "df_hastarget.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `AveragePriceNextWeek`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 `AveragePrice` baseline \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "Soon we will want to build some models to forecast the average avocado price a week in advance. Before we start with any ML though, let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get on the train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train: 0.82858\n"
     ]
    }
   ],
   "source": [
    "train_r2 = r2_score(df_train[\"AveragePriceNextWeek\"], df_train[\"AveragePrice\"])\n",
    "print(\"R^2 train: %.5f\" % train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 test: 0.76318\n"
     ]
    }
   ],
   "source": [
    "test_r2 = r2_score(df_test[\"AveragePriceNextWeek\"], df_test[\"AveragePrice\"])\n",
    "print(\"R^2 test: %.5f\" % test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Forecasting average avocado price\n",
    "\n",
    "_Points:_ 10\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "Benchmark: you should be able to achieve $R^2$ of at least 0.79 on the test set. We got to 0.80, but not beyond that. Let us know if you do better!\n",
    "\n",
    "Note: because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15441 entries, 0 to 18222\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   Date                  15441 non-null  datetime64[ns]\n",
      " 1   AveragePrice          15441 non-null  float64       \n",
      " 2   Total Volume          15441 non-null  float64       \n",
      " 3   4046                  15441 non-null  float64       \n",
      " 4   4225                  15441 non-null  float64       \n",
      " 5   4770                  15441 non-null  float64       \n",
      " 6   Total Bags            15441 non-null  float64       \n",
      " 7   Small Bags            15441 non-null  float64       \n",
      " 8   Large Bags            15441 non-null  float64       \n",
      " 9   XLarge Bags           15441 non-null  float64       \n",
      " 10  type                  15441 non-null  object        \n",
      " 11  year                  15441 non-null  int64         \n",
      " 12  region                15441 non-null  object        \n",
      " 13  AveragePriceNextWeek  15441 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(10), int64(1), object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"AveragePrice\",\n",
    "    \"Total Volume\",\n",
    "    \"4046\",\n",
    "    \"4225\",\n",
    "    \"4770\",\n",
    "    \"Total Bags\",\n",
    "    \"Small Bags\",\n",
    "    \"Large Bags\",\n",
    "    \"XLarge Bags\",\n",
    "    \"year\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \"type\",\n",
    "    \"region\",\n",
    "]\n",
    "drop_features = [\"Date\"]\n",
    "target = [\"AveragePriceNextWeek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Lecture 19\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "def preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "\n",
    "    all_features = set(numeric_features + categorical_features + drop_features + target)\n",
    "    if set(train_df.columns) != all_features:\n",
    "        print(\"Missing columns\", set(train_df.columns) - all_features)\n",
    "        print(\"Extra columns\", all_features - set(train_df.columns))\n",
    "        raise Exception(\"Columns do not match\")\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    "    )\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (numeric_transformer, numeric_features),\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(train_df)\n",
    "    ohe_feature_names = (\n",
    "        preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "    new_columns = numeric_features + ohe_feature_names\n",
    "\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(test_df), index=test_df.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = train_df[\"AveragePriceNextWeek\"]\n",
    "    y_test = test_df[\"AveragePriceNextWeek\"]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from lecture 19\n",
    "\n",
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, Ridge())\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"ridge\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1) No Date Encoding - default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.328682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.087368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.084933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_NewYork</th>\n",
       "      <td>0.067025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_organic</th>\n",
       "      <td>0.049510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.045930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_conventional</th>\n",
       "      <td>-0.049510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.065811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.065815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.075620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.328682\n",
       "region_SanFrancisco         0.087368\n",
       "region_HartfordSpringfield  0.084933\n",
       "region_NewYork              0.067025\n",
       "type_organic                0.049510\n",
       "...                              ...\n",
       "region_Denver              -0.045930\n",
       "type_conventional          -0.049510\n",
       "region_DallasFtWorth       -0.065811\n",
       "region_SouthCentral        -0.065815\n",
       "region_Houston             -0.075620\n",
       "\n",
       "[66 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from Lecture 19\n",
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")\n",
    "\n",
    "score_lr_print_coeff(preprocessor, df_train, y_train, df_test, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2) Encoding time as a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.78\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.324498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.091838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.089592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_NewYork</th>\n",
       "      <td>0.070944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Philadelphia</th>\n",
       "      <td>0.052258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.047782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_conventional</th>\n",
       "      <td>-0.052177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.069429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.070072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.079556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.324498\n",
       "region_SanFrancisco         0.091838\n",
       "region_HartfordSpringfield  0.089592\n",
       "region_NewYork              0.070944\n",
       "region_Philadelphia         0.052258\n",
       "...                              ...\n",
       "region_Denver              -0.047782\n",
       "type_conventional          -0.052177\n",
       "region_DallasFtWorth       -0.069429\n",
       "region_SouthCentral        -0.070072\n",
       "region_Houston             -0.079556\n",
       "\n",
       "[67 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adapted from lecture 19\n",
    "first_day = df_train[\"Date\"].min()\n",
    "\n",
    "train_df = df_train.assign(\n",
    "    Days_since=df_train[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")\n",
    "test_df = df_test.assign(\n",
    "    Days_since=df_test[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")\n",
    "\n",
    "train_df.sort_values(by=\"Date\").head()\n",
    "\n",
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features + [\"Days_since\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")\n",
    "\n",
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3) One-hot encoding of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.316943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.247612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_September</th>\n",
       "      <td>0.112086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_October</th>\n",
       "      <td>0.100772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.100081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.086358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_April</th>\n",
       "      <td>-0.087222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_February</th>\n",
       "      <td>-0.120834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_January</th>\n",
       "      <td>-0.179477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Days_since</th>\n",
       "      <td>-0.230822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Coef\n",
       "AveragePrice         0.316943\n",
       "year                 0.247612\n",
       "Month_September      0.112086\n",
       "Month_October        0.100772\n",
       "region_SanFrancisco  0.100081\n",
       "...                       ...\n",
       "region_Houston      -0.086358\n",
       "Month_April         -0.087222\n",
       "Month_February      -0.120834\n",
       "Month_January       -0.179477\n",
       "Days_since          -0.230822\n",
       "\n",
       "[79 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.assign(\n",
    "    Month=train_df[\"Date\"].apply(lambda x: x.month_name())\n",
    ")  # x.month_name() to get the actual string\n",
    "test_df = test_df.assign(Month=test_df[\"Date\"].apply(lambda x: x.month_name()))\n",
    "\n",
    "train_df[[\"Date\", \"Month\"]].sort_values(by=\"Month\")\n",
    "\n",
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features + [\"Days_since\"],\n",
    "    categorical_features + [\"Month\"],\n",
    "    drop_features,\n",
    "    target\n",
    ")\n",
    "\n",
    "score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach I tried was a default without encoding the date. This achieved a test score of 0.79.\n",
    "The second approach was encoding time as a number from the split date to see if the date would help predict the target. The test score was 0.78, which was worse than the score achieved with the default.\n",
    "The third approach was to add one-hot encoding of the month for the Date field since individual days were not useful in predicting the target. This method achieved a test score of 0.80.\n",
    "\n",
    "There is a lot of variability on a day to day basis, so it makes sense that the second approach yielded the worst results. By aggregating days into months, as in the third approach, the data became normalized, enabling more general patterns to be extracted by the model. Further, the seasonality of avocados is better modelled on a month-to-month basis rather than day-to-day. Thus, it makes sense that the third approach had the best test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: very short answer questions\n",
    "\n",
    "Each question is worth 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Time series\n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "The following questions pertain to Lecture 19 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Environmental monitoring systems collect data on parameters such as air quality, water quality, and pollution levels at irregular intervals based on factors such as sensor locations, sampling frequency, and environmental events. For example, air quality sensors might record data more frequently in urban areas with high pollution levels or during events such as wildfires, while data collection might be less frequent in rural areas with lower pollution levels. As a result, the time series data would have unequally spaced time points, reflecting the variability in environmental conditions and monitoring practices.\n",
    "\n",
    "2. Both approaches would struggle with unequally spaced time points. Encoding the data as features assumes that the interval between time points is consistent, so when the time points are irregularly spaced the model may develop inaccurate interpretatations of temporal patterns. Creating lagged version of features involves creating lagged versions of features relative to a fixed lag time. If time points are unevenly spaced, the lagged version may not align correctly with the corresponding target observations, leading to inaccuracies in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Survival analysis\n",
    "\n",
    "_Points:_ 6\n",
    "\n",
    "The following questions pertain to Lecture 20 on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer?\n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLEASE READ BEFORE YOU SUBMIT:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using [PrairieLearn](https://ca.prairielearn.com/pl/course_instance/6697). Don't forget to rename your file `hw8_sol.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
