{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 6: Clustering\n",
    "\n",
    "**Associated lectures:** [Lectures 14 and 15](https://github.com/UBC-CS/cpsc330-2023W2/tree/main/lectures).\n",
    "- We will use the concept of embeddings from [Lectures 16](https://github.com/UBC-CS/cpsc330-2023W2/tree/main/lectures). You can use it as a tool and do not need Lecture 16 to do the assignment, but feel free to take a look to get a better understanding!\n",
    "\n",
    "**Due date: [Mar 25, 11:59pm](https://github.com/UBC-CS/cpsc330-2023W2?tab=readme-ov-file#deliverable-due-dates-tentative).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a name=\"im\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions <a name=\"si\"></a>\n",
    "<hr>\n",
    "\n",
    "_points: 2_\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. To submit this assignment, follow the instructions below:\n",
    "\n",
    "- **You may work on this assignment in a group (group size $\\leq$ 4) and submit your assignment as a group.** \n",
    "- Below are some instructions on working as a group.  \n",
    "    - The maximum group size is 4. \n",
    "    - You can choose your own group members. \n",
    "    - Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "    - Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "    - It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. [Here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members) are some instructions on adding group members in Gradescope.  \n",
    "- Be sure to follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W2/blob/main/docs/homework_instructions.md).\n",
    "- Upload the .ipynb file to PrairieLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8341"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.randint(100, 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Document clustering warm-up\n",
    "<hr>\n",
    "\n",
    "In this homework, we will explore a popular application of clustering called [**document clustering**](https://en.wikipedia.org/wiki/Document_clustering). A large amount of unlabeled text data is available out there (e.g., news, recipes, online Q&A, tweets), and clustering is a commonly used technique to organize this data in a meaningful way. \n",
    "\n",
    "As a warm up, in this exercise, you will cluster sentences from a toy corpus. Later in the homework, you will work with a real corpus. \n",
    "\n",
    "The code below extracts introductory sentences of Wikipedia articles on a set of queries. To run the code successfully, you will need the `wikipedia` package installed in the course environment. \n",
    "\n",
    "```\n",
    "conda activate cpsc330\n",
    "conda install -c conda-forge wikipedia\n",
    "```\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Run the code below which \n",
    "- extracts content of Wikipedia articles on a set of queries\n",
    "- tokenizes the text (i.e., separates sentences) and \n",
    "- stores the 2nd sentence in each article as a document representing that article\n",
    "\n",
    "> Feel free to experiment with Wikipedia queries of your choice. But stick to the provided list for the final submission so that it's easier for the TAs to grade your submission.\n",
    "\n",
    "> For tokenization we are using the `nltk` package. If you do not have this package in the course environment, you will have to install it.\n",
    "\n",
    "```\n",
    "conda activate cpsc330\n",
    "conda install -c anaconda nltk\n",
    "```\n",
    "\n",
    "Even if you have the package installed via the course `conda` environment, you might have to download `nltk` pre-trained models, which can be done with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\linda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hummus food</td>\n",
       "      <td>The standard garnish in the Middle East includes olive oil, a few whole chickpeas, parsley, and paprika.The earliest mention of Hummus comes from Syria, in a 13th-century cookbook attributed to the Aleppine historian Ibn al-Adim.Popular in Middle Eastern cuisine, it is usually eaten as a dip, with pita bread.</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bread food</td>\n",
       "      <td>Throughout recorded history and around the world, it has been an important part of many cultures' diet.</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>As a field of research in computer science focusing on the automation of intelligent behavior through machine learning, it develops and studies methods and software which enable machines to perceive their environment and take actions that maximize their chances of achieving defined goals, with the aim of performing tasks typically associated with human intelligence.</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>The training data is processed, building a function that maps new data on expected output values.</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>football sport</td>\n",
       "      <td>Sports can, through casual or organized participation, improve participants' physical health.</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>It belongs to a family of sports called hockey.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                wiki query  \\\n",
       "0  hummus food               \n",
       "1  bread food                \n",
       "2  artificial intelligence   \n",
       "3  unsupervised learning     \n",
       "4  football sport            \n",
       "5  ice hockey                \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "0  The standard garnish in the Middle East includes olive oil, a few whole chickpeas, parsley, and paprika.The earliest mention of Hummus comes from Syria, in a 13th-century cookbook attributed to the Aleppine historian Ibn al-Adim.Popular in Middle Eastern cuisine, it is usually eaten as a dip, with pita bread.                                                             \n",
       "1  Throughout recorded history and around the world, it has been an important part of many cultures' diet.                                                                                                                                                                                                                                                                            \n",
       "2  As a field of research in computer science focusing on the automation of intelligent behavior through machine learning, it develops and studies methods and software which enable machines to perceive their environment and take actions that maximize their chances of achieving defined goals, with the aim of performing tasks typically associated with human intelligence.   \n",
       "3  The training data is processed, building a function that maps new data on expected output values.                                                                                                                                                                                                                                                                                  \n",
       "4  Sports can, through casual or organized participation, improve participants' physical health.                                                                                                                                                                                                                                                                                      \n",
       "5  It belongs to a family of sports called hockey.                                                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "   n_words  \n",
       "0  56       \n",
       "1  20       \n",
       "2  57       \n",
       "3  18       \n",
       "4  15       \n",
       "5  10       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "queries = [\n",
    "    \"hummus food\",\n",
    "    \"bread food\",\n",
    "    \"artificial intelligence\",\n",
    "    \"unsupervised learning\",\n",
    "    \"football sport\",\n",
    "    \"ice hockey\",\n",
    "]\n",
    "\n",
    "wiki_dict = {\"wiki query\": [], \"text\": [], \"n_words\": []}\n",
    "for i in range(len(queries)):\n",
    "    text = sent_tokenize(wikipedia.page(queries[i]).content)[1]\n",
    "    wiki_dict[\"text\"].append(text)\n",
    "    wiki_dict[\"n_words\"].append(len(word_tokenize(text)))\n",
    "    wiki_dict[\"wiki query\"].append(queries[i])\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_dict)\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our toy corpus has six toy documents (`text` column in the dataframe) extracted from Wikipedia queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many clusters? \n",
    "\n",
    "_Points:_ 1\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. If you are asked to cluster the documents from this toy corpus manually, how many clusters would you identify and how would you label each cluster?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would identify 3 clusters labelled \"food\", \"sports\", and \"data science\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 `KMeans` with bag-of-words representation \n",
    "\n",
    "_Points:_ 3\n",
    "\n",
    "Before we pass text to machine learning models, we need to encode it to a numeric representation. We will try two representations: bag-of-words representation and sentence embedding representation. First, let's try our good old friend, bag-of-words.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create bag-of-words representation using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) with `stop_words='english'` for the `text` column in `wiki_df` above. Store the representation in `transformed`.\n",
    "2. Cluster the documents with this representation and [`KMeans` clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) with the following arguments: \n",
    "    - `random_state=42` (for reproducibility)\n",
    "    - `n_clusters`=the number of clusters you identified in 1.1\n",
    "3. Store the labels as a list in `kmeans_labels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words='english') # A variable for `CountVectorizer` object\n",
    "transformed = countvec.fit_transform(wiki_df[\"text\"]) # A variable for the BOW representation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_wiki = KMeans(random_state=42, n_clusters=3) # KMeans object\n",
    "kmeans_wiki.fit(transformed)\n",
    "kmeans_labels = kmeans_wiki.predict(transformed) # list\n",
    "kmeans_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sentence embedding representation\n",
    "\n",
    "_Points:_ 3\n",
    "\n",
    "As we have seen before, bag-of-words representation is limited in that it does not take into account word ordering and context. There are other richer and more expressive representations of text, and we are going to use one such representation in this homework. We will call it **sentence embedding representation**. We'll use [sentence transformer](https://www.sbert.net/index.html) to extract these representations. At this point it's enough to know that this is an alternative representation of text which usually works better than simple bag-of-words representation. We will talk a bit more about embedding representations in the coming weeks. To use sentence transformer, you need to install `sentence-transformers` in the course conda environment to run the code below. \n",
    "\n",
    "```conda install -c conda-forge sentence-transformers```\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Run the code below to create sentence embedding representation of documents in our toy corpus. \n",
    "2. Cluster the documents with this representation (`emb_sents`) and `KMeans` with the following arguments: \n",
    "    - `random_state=42` (for reproducibility)\n",
    "    - `n_clusters`=the number of clusters you identified in 1.1\n",
    "3. Store the labels as a list in `kmeans_emb_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c667a24a4d4cb6a720e8da8380245a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\linda\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-distilroberta-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5e1d5da191405fbf2d24ed4e6357aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eebe3f41bc14a9ab7ab46e7bdbb7b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0146e18214e411bbe2b8b13eed8bbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8432d10b1b684417af00d6c0af98f427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf0f4eec78342898041bf7348e8440b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2743b5d74e40d8b695f386bf7828a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab45bdf4ed242e78983a5e235f4237d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855d3b57dd2d44629f5da11face7e4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff19207616bd449c8c5b6c63bfc05cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a59e2c74ec4423780f0902e042385a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af66c60c87524e258fc3dbf2c152e25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"paraphrase-distilroberta-base-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.186445</td>\n",
       "      <td>0.053468</td>\n",
       "      <td>-0.085373</td>\n",
       "      <td>0.142412</td>\n",
       "      <td>0.292351</td>\n",
       "      <td>0.111269</td>\n",
       "      <td>-0.026790</td>\n",
       "      <td>-0.284755</td>\n",
       "      <td>-0.052542</td>\n",
       "      <td>0.224934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239208</td>\n",
       "      <td>-0.011231</td>\n",
       "      <td>0.287064</td>\n",
       "      <td>0.084698</td>\n",
       "      <td>-0.050105</td>\n",
       "      <td>-0.100315</td>\n",
       "      <td>0.560232</td>\n",
       "      <td>0.243171</td>\n",
       "      <td>0.077927</td>\n",
       "      <td>-0.057998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022418</td>\n",
       "      <td>0.217159</td>\n",
       "      <td>0.022694</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.240856</td>\n",
       "      <td>0.358046</td>\n",
       "      <td>-0.053310</td>\n",
       "      <td>-0.328075</td>\n",
       "      <td>0.190012</td>\n",
       "      <td>0.244470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265412</td>\n",
       "      <td>-0.415594</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.052483</td>\n",
       "      <td>0.345947</td>\n",
       "      <td>0.110091</td>\n",
       "      <td>0.405441</td>\n",
       "      <td>0.197792</td>\n",
       "      <td>-0.058254</td>\n",
       "      <td>0.212376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126692</td>\n",
       "      <td>0.186392</td>\n",
       "      <td>0.150296</td>\n",
       "      <td>0.382365</td>\n",
       "      <td>0.450249</td>\n",
       "      <td>-0.161789</td>\n",
       "      <td>0.084148</td>\n",
       "      <td>0.153293</td>\n",
       "      <td>0.255594</td>\n",
       "      <td>-0.223556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329319</td>\n",
       "      <td>-0.092291</td>\n",
       "      <td>-0.119608</td>\n",
       "      <td>0.053409</td>\n",
       "      <td>-0.128812</td>\n",
       "      <td>-0.054778</td>\n",
       "      <td>0.680740</td>\n",
       "      <td>-0.087321</td>\n",
       "      <td>0.033664</td>\n",
       "      <td>0.037519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258627</td>\n",
       "      <td>-0.285747</td>\n",
       "      <td>-0.248419</td>\n",
       "      <td>0.297078</td>\n",
       "      <td>0.214667</td>\n",
       "      <td>0.037092</td>\n",
       "      <td>-0.100457</td>\n",
       "      <td>0.057128</td>\n",
       "      <td>0.279170</td>\n",
       "      <td>0.049970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493944</td>\n",
       "      <td>-0.090611</td>\n",
       "      <td>0.320196</td>\n",
       "      <td>0.152408</td>\n",
       "      <td>-0.233400</td>\n",
       "      <td>0.105795</td>\n",
       "      <td>0.443150</td>\n",
       "      <td>-0.616379</td>\n",
       "      <td>0.254020</td>\n",
       "      <td>0.164758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.401523</td>\n",
       "      <td>0.294518</td>\n",
       "      <td>0.169535</td>\n",
       "      <td>0.092036</td>\n",
       "      <td>0.159018</td>\n",
       "      <td>0.504919</td>\n",
       "      <td>0.392199</td>\n",
       "      <td>-0.074318</td>\n",
       "      <td>0.314625</td>\n",
       "      <td>0.033591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362093</td>\n",
       "      <td>-0.135943</td>\n",
       "      <td>0.133036</td>\n",
       "      <td>0.353604</td>\n",
       "      <td>-0.222303</td>\n",
       "      <td>0.077625</td>\n",
       "      <td>-0.082624</td>\n",
       "      <td>0.261559</td>\n",
       "      <td>0.272954</td>\n",
       "      <td>0.113219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.108901</td>\n",
       "      <td>0.087363</td>\n",
       "      <td>0.119847</td>\n",
       "      <td>-0.050133</td>\n",
       "      <td>0.240210</td>\n",
       "      <td>-0.082101</td>\n",
       "      <td>0.190988</td>\n",
       "      <td>0.436203</td>\n",
       "      <td>-0.085557</td>\n",
       "      <td>-0.012788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145415</td>\n",
       "      <td>0.160033</td>\n",
       "      <td>0.071930</td>\n",
       "      <td>0.147640</td>\n",
       "      <td>0.043789</td>\n",
       "      <td>0.253031</td>\n",
       "      <td>-0.009358</td>\n",
       "      <td>0.415206</td>\n",
       "      <td>0.142142</td>\n",
       "      <td>0.130365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.186445  0.053468 -0.085373  0.142412  0.292351  0.111269 -0.026790   \n",
       "1 -0.022418  0.217159  0.022694  0.003616  0.240856  0.358046 -0.053310   \n",
       "2  0.126692  0.186392  0.150296  0.382365  0.450249 -0.161789  0.084148   \n",
       "3  0.258627 -0.285747 -0.248419  0.297078  0.214667  0.037092 -0.100457   \n",
       "4  0.401523  0.294518  0.169535  0.092036  0.159018  0.504919  0.392199   \n",
       "5  0.108901  0.087363  0.119847 -0.050133  0.240210 -0.082101  0.190988   \n",
       "\n",
       "          7         8         9  ...       758       759       760       761  \\\n",
       "0 -0.284755 -0.052542  0.224934  ...  0.239208 -0.011231  0.287064  0.084698   \n",
       "1 -0.328075  0.190012  0.244470  ...  0.265412 -0.415594  0.003036  0.052483   \n",
       "2  0.153293  0.255594 -0.223556  ...  0.329319 -0.092291 -0.119608  0.053409   \n",
       "3  0.057128  0.279170  0.049970  ...  0.493944 -0.090611  0.320196  0.152408   \n",
       "4 -0.074318  0.314625  0.033591  ...  0.362093 -0.135943  0.133036  0.353604   \n",
       "5  0.436203 -0.085557 -0.012788  ...  0.145415  0.160033  0.071930  0.147640   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0 -0.050105 -0.100315  0.560232  0.243171  0.077927 -0.057998  \n",
       "1  0.345947  0.110091  0.405441  0.197792 -0.058254  0.212376  \n",
       "2 -0.128812 -0.054778  0.680740 -0.087321  0.033664  0.037519  \n",
       "3 -0.233400  0.105795  0.443150 -0.616379  0.254020  0.164758  \n",
       "4 -0.222303  0.077625 -0.082624  0.261559  0.272954  0.113219  \n",
       "5  0.043789  0.253031 -0.009358  0.415206  0.142142  0.130365  \n",
       "\n",
       "[6 rows x 768 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_sents = embedder.encode(wiki_df[\"text\"])\n",
    "emb_sent_df = pd.DataFrame(emb_sents, index=wiki_df.index)\n",
    "emb_sent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 2, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_emb = KMeans(random_state=42, n_clusters=3) # KMeans object\n",
    "kmeans_emb.fit(emb_sents)\n",
    "kmeans_emb_labels = kmeans_emb.labels_.tolist() # list\n",
    "kmeans_emb_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 DBSCAN with cosine distance  \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "Let's try [`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on our toy dataset. K-Means is kind of bound to the Euclidean distance because it is based on the notion of means. With `DBSCAN` we can try different distance metrics. In the context of text, [cosine similarities](https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) or cosine distances tend to work better. Given vectors $u$ and $v$, the **cosine distance** between the vectors is defined as: \n",
    "\n",
    "$$distance_{cosine}(u,v) = 1 - (\\frac{u \\cdot v}{\\left\\lVert u\\right\\rVert_2 \\left\\lVert v\\right\\rVert_2})$$\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Cluster documents in our toy corpus encoded with sentence embedding representation (`emb_sents`) and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?highlight=dbscan#sklearn.cluster.DBSCAN) with `metric='cosine'`. \n",
    "\n",
    "> *Note: You will have to set appropriate values for the hyperparameters `eps` and `min_samples` to get meaningful clusters, as default values for these hyperparameters are unlikely to work on this toy dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_scan  = DBSCAN(metric='cosine', eps=0.8, min_samples=2)\n",
    "db_scan.fit(emb_sents)\n",
    "db_scan_labels = db_scan.labels_.tolist() # list\n",
    "db_scan_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Discussion\n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Reflect and comment on the clusters identified by each of the methods you explored in 1.2, 1.3, and 1.4. Are these methods doing a reasonable job in clustering the sentences in our toy corpus? Do the clustering results change with the representation you use?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.6 Visualizing clusters\n",
    "\n",
    "_Points:_ 3\n",
    "\n",
    "One thing we can do with unlabeled data is visualizing it. That said, our data is high dimensional and high-dimensional data is hard to visualize. For example, in sentence embedding representation, each example is represented with 768 dimensions. One way to visualize high-dimensional data is by applying dimensionality reduction to get the most important (2 or 3) components of the dataset and visualizing this low-dimensional data. \n",
    "\n",
    "Given data as a `numpy` array and corresponding cluster assignments, the `plot_umap_clusters` function below transforms the data by applying dimensionality reduction technique called [UMAP](https://umap-learn.readthedocs.io/en/latest/) to it and plots the transformed data with different colours for different clusters. \n",
    "\n",
    "> *Note: At this point we are using this function only for visualization and you are not expected to understand the UMAP part.* \n",
    "\n",
    "You'll have to install the `umap-learn` package in the course conda environment either with `conda` or `pip`, as described in the [documentation](https://umap-learn.readthedocs.io/en/latest/index.html). \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c conda-forge umap-learn\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> pip install umap-learn\n",
    "```\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Visualize the clusters created by the three methods above using `plot_umap_clusters` function below:\n",
    "    - KMeans with bag-of-words representation \n",
    "    - KMeans with sentence embedding representation\n",
    "    - DBSCAN with sentence embedding representation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_clusters(\n",
    "    data,\n",
    "    cluster_labels,\n",
    "    raw_sents=wiki_df[\"text\"],\n",
    "    show_labels=False,\n",
    "    size=50,\n",
    "    n_neighbors=15,\n",
    "    title=\"UMAP visualization\",\n",
    "    ignore_noise=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Carry out dimensionality reduction using UMAP and plot 2-dimensional clusters.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data : numpy array\n",
    "        data as a numpy array\n",
    "    cluster_labels : list\n",
    "        cluster labels for each row in the dataset\n",
    "    raw_sents : list\n",
    "        the original raw sentences for labeling datapoints\n",
    "    show_labels : boolean\n",
    "        whether you want to show labels for points or not (default: False)\n",
    "    size : int\n",
    "        size of points in the scatterplot\n",
    "    n_neighbors : int\n",
    "        n_neighbors hyperparameter of UMAP. See the documentation.\n",
    "    title : str\n",
    "        title for the visualization plot\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    None. Shows the clusters.\n",
    "    \"\"\"\n",
    "\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, random_state=42)\n",
    "    Z = reducer.fit_transform(data)  # reduce dimensionality\n",
    "    umap_df = pd.DataFrame(data=Z, columns=[\"dim1\", \"dim2\"])\n",
    "    umap_df[\"cluster\"] = cluster_labels\n",
    "\n",
    "    if ignore_noise:\n",
    "        umap_df = umap_df[umap_df[\"cluster\"] != -1]\n",
    "\n",
    "    labels = np.unique(umap_df[\"cluster\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        umap_df[\"dim1\"],\n",
    "        umap_df[\"dim2\"],\n",
    "        c=umap_df[\"cluster\"],\n",
    "        cmap=\"tab20b\",\n",
    "        s=size,\n",
    "        edgecolors=\"k\",\n",
    "        linewidths=0.1,\n",
    "    )\n",
    "\n",
    "    legend = ax.legend(*scatter.legend_elements(), loc=\"best\", title=\"Clusters\")\n",
    "    ax.add_artist(legend)\n",
    "\n",
    "    if show_labels:\n",
    "        x = umap_df[\"dim1\"].tolist()\n",
    "        y = umap_df[\"dim2\"].tolist()\n",
    "        for i, txt in enumerate(raw_sents):\n",
    "            ax.annotate(\" \".join(txt.split()[:10]), (x[i], y[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: [Food.com](https://www.food.com/) recipes \n",
    "<hr>\n",
    "\n",
    "Now that we have applied document clustering on a toy dataset, let's cluster sentences from a real corpus. In this lab we will work with a sample of [Kaggle's Food.com recipes corpus]( https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions). This corpus contains 180K+ recipes and 700K+ recipe reviews. In this homework, we'll only focus on recipes and **not** on recipe reviews. The recipes are present in `RAW_recipes.csv`. Our goal is to find main categories or groupings of recipes based on their names. \n",
    "\n",
    "**Your tasks:**\n",
    "- Download [`RAW_recipes.csv`](https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions?select=RAW_recipes.csv) and put it in the homework folder under the data folder. As usual, do not push the CSV in your repository. \n",
    "- Run the code below. The dataset is quite large, and in this assignment, for speed, you will work with a sample of the dataset. The function `get_recipes_sample` below carries out some preliminary preprocessing and returns a sample of the recipes with most frequent tags. \n",
    "\n",
    "> *Note: Depending upon the capacity of your computer, feel free to increase or decrease the size of this sample by changing the value for `n_tags`.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_recipes_df = pd.read_csv(\"data/RAW_recipes.csv\")\n",
    "orig_recipes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipes_sample(orig_recipes_df, n_tags=300, min_len=5):\n",
    "    orig_recipes_df = orig_recipes_df.dropna()  # Remove rows with NaNs.\n",
    "    orig_recipes_df = orig_recipes_df.drop_duplicates(\n",
    "        \"name\"\n",
    "    )  # Remove rows with duplicate names.\n",
    "    # Remove rows where recipe names are too short (< 5 characters).\n",
    "    orig_recipes_df = orig_recipes_df[orig_recipes_df[\"name\"].apply(len) >= min_len]\n",
    "    # Only consider the rows where tags are one of the most frequent n tags.\n",
    "    first_n = orig_recipes_df[\"tags\"].value_counts()[0:n_tags].index.tolist()\n",
    "    recipes_df = orig_recipes_df[orig_recipes_df[\"tags\"].isin(first_n)]\n",
    "    return recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df = get_recipes_sample(orig_recipes_df)\n",
    "recipes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the rest of the homework, we will use `recipes_df` above, which is a subset of the original dataset.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Longest and shorted recipe names \n",
    "\n",
    "_Points:_ 2\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Print the shortest and longest recipe names (length in terms of number of characters) from `recipes_df`. If there is more than one recipe with the same shortest/longest length, store **one** in `shortest_recipe` and/or `longest_recipe` as a **string**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortest_recipe = None\n",
    "longest_recipe = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 More EDA\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Create a word cloud for the recipe names. You can use [the `wordcloud` package](https://github.com/amueller/word_cloud) for this, which you will have to install in the course environment. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c conda-forge wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Representing recipe names\n",
    "\n",
    "_Points:_ 3\n",
    "\n",
    "The next step is creating a representation of recipe names. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Similar to Exercise 1, create sentence embedding representation of recipe names (`name` column in `recipes_df`).  For the rest of the homework, we'll stick to the sentence embedding representation of recipe names.\n",
    "\n",
    "\n",
    "\n",
    "> *If you create a dataframe with sentence embedding representation, set the index to `recipes_df.index` so that the indices match with the indices of the sample we are working with.*  \n",
    "> *This might take a while to run.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: K-Means on Food.com recipe names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.1 Choosing K for K-Means\n",
    "\n",
    "_Points:_ 6\n",
    "\n",
    "For K-Means you need to specify the number of clusters in advance, which is often challenging to do on real datasets. As we saw in the lecture, there is no definitive method to select the number of clusters. That said, there are some approaches which may help us with this process. In this exercise, you'll explore three such approaches. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Consider a reasonable range of K (`n_clusters`) values and visualize the Elbow plot. \n",
    "2. Consider a reasonable range of K (`n_clusters`) and visualize the clusters created by K-Means by using `plot_umap_clusters` function from Exercise 1. \n",
    "\n",
    "> You may use the [`yellowbrick`](https://www.scikit-yb.org/en/latest/) package for visualizing the Elbow plot.   \n",
    "\n",
    "```\n",
    "pip install yellowbrick\n",
    "```\n",
    "\n",
    "> The range of K or `n_clusters` values does not have to be the same in the cases above. \n",
    "\n",
    "> Use the default value of `show_labels=False` when you call function `plot_umap_clusters`, as we do not want to display labels of thousands of data points.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2 Discussion \n",
    "\n",
    "_Points:_ 4\n",
    "\n",
    "**Your tasks:** \n",
    "1. Comment on your results from 3.1. Are the plots above useful in narrowing down the range of values for `n_clusters`? Based on these visualizations, what value or a range of values seems reasonable for `n_clusters` in this problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3 Sampling recipe names from clusters\n",
    "\n",
    "_Points:_ 5\n",
    "\n",
    "It's likely that with the methods in the previous exercises you did not get a satisfactory answer on how many clusters should be appropriate for this problem. One of the most important steps in clustering is manual interpretation of clusters. In this exercise, you will examine some samples from different clusters given by K-Means, which might give you a better understanding on the number of clusters and whether the clusters make sense or not.  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Based on your answer in Exercise 3.1 and 3.2, pick one or two reasonable values for `n_clusters` and train `KMeans` with those values and `random_state=42`. \n",
    "2. Sample some examples (e.g., 10 to 15 recipe names) from each cluster and show the sampled recipes for each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.4 Manual interpretation of clusters\n",
    "\n",
    "_Points:_ 5\n",
    "\n",
    "**Your tasks:**\n",
    "1. Do you see a clear distinction between clusters? What topics/themes do to see in different clusters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.5 Dendrogram\n",
    "\n",
    "_Points:_ 3\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Show a dendrogram with `p=10` and `truncate_mode=level` on sentence embeddings of recipes with average linkage and `metric=\"cosine\"`.\n",
    "2. Briefly comment on the results.\n",
    "\n",
    "> *Note: Try orientation=\"left\" of `dendrogram` for better readability of the dendrogram.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLEASE READ BEFORE YOU SUBMIT:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using [PrairieLearn](https://ca.prairielearn.com/pl/course_instance/6697). Don't forget to rename your file `hw6_sol.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
