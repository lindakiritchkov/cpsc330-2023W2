{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Machine Learning Fundamentals \n",
    "\n",
    "UBC 2023-24\n",
    "\n",
    "Instructors: Mathias LÃ©cuyer and Mehrdad Oveisi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- hw2 is released. (Due next week Monday, Jan 22, at 11:59pm)\n",
    "    - You are welcome to broadly discuss it with your classmates but final answers and submissions must be your own. \n",
    "    - Group submissions are **not** allowed for this assignment.\n",
    "- Advice on keeping up with the material \n",
    "    - Practice! \n",
    "    - Make sure you run the lecture notebooks on your laptop. \n",
    "    - Start early on homework assignments.     \n",
    "- If you are still on the waitlist, **it's your responsibility to keep up with the material and submit assignments**. You can ask for help to submit via cpsc330-admin@cs.ubc.ca\n",
    "- Last day to withdraw without a W standing: Mon Jan 22, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Announcements, LOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:51.225391Z",
     "iopub.status.busy": "2024-01-16T21:38:51.223938Z",
     "iopub.status.idle": "2024-01-16T21:38:52.745608Z",
     "shell.execute_reply": "2024-01-16T21:38:52.744361Z",
     "shell.execute_reply.started": "2024-01-16T21:38:51.225359Z"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "from plotting_functions import *\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning outcomes\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- explain how decision boundaries change with the `max_depth` hyperparameter;\n",
    "- explain the concept of generalization;\n",
    "- appropriately split a dataset into train and test sets using `train_test_split` function;\n",
    "- explain the difference between train, validation, test, and \"deployment\" data;\n",
    "- identify the difference between training error, validation error, and test error;\n",
    "- explain cross-validation and use `cross_val_score` and `cross_validate` to calculate cross-validation error;\n",
    "- recognize overfitting and/or underfitting by looking at train and test scores;\n",
    "- explain why it is generally not possible to get a perfect test score (zero test error) on a supervised learning problem;\n",
    "- describe the fundamental tradeoff between training score and the train-test gap;\n",
    "- state the golden rule;\n",
    "- start to build a standard recipe for supervised learning: train/test split, hyperparameter tuning with cross-validation, test on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization [[video](https://youtu.be/iS2hsRRlc2M)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Big picture and motivation \n",
    "\n",
    "In machine learning, we want to glean information from labeled data so that we can label **new unlabeled** data. For example, suppose we want to build a spam filtering system.  We will take a large number of spam/non-spam messages from the past, learn patterns associated with spam/non-spam from them, and predict whether **a new incoming message** in someone's inbox is spam or non-spam based on these patterns. \n",
    "\n",
    "So we want to **learn from the past** but ultimately we want to apply it on the **future email messages**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/eva-think.png)\n",
    "\n",
    "**How can we generalize from what we've seen to what we haven't seen?** \n",
    "\n",
    "In this lecture, we'll see how machine learning tackles this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model complexity and training error\n",
    "\n",
    "In the last lecture, we looked at **decision boundaries**, a way to visualize what sort of examples will be classified as positive and negative. \n",
    "\n",
    "Let's examine how does the decision boundary change for different tree depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:52.748008Z",
     "iopub.status.busy": "2024-01-16T21:38:52.747511Z",
     "iopub.status.idle": "2024-01-16T21:38:52.776122Z",
     "shell.execute_reply": "2024-01-16T21:38:52.774505Z",
     "shell.execute_reply.started": "2024-01-16T21:38:52.747985Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/quiz2-grade-toy-classification.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Toy quiz2 grade data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m classification_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/quiz2-grade-toy-classification.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m classification_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\linda\\miniconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/quiz2-grade-toy-classification.csv'"
     ]
    }
   ],
   "source": [
    "# Toy quiz2 grade data\n",
    "classification_df = pd.read_csv(\"data/quiz2-grade-toy-classification.csv\")\n",
    "classification_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:52.779365Z",
     "iopub.status.busy": "2024-01-16T21:38:52.777686Z",
     "iopub.status.idle": "2024-01-16T21:38:52.789764Z",
     "shell.execute_reply": "2024-01-16T21:38:52.787918Z",
     "shell.execute_reply.started": "2024-01-16T21:38:52.779232Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = classification_df.drop([\"quiz2\"], axis=1)\n",
    "y = classification_df[\"quiz2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:52.792079Z",
     "iopub.status.busy": "2024-01-16T21:38:52.791534Z",
     "iopub.status.idle": "2024-01-16T21:38:52.808489Z",
     "shell.execute_reply": "2024-01-16T21:38:52.806579Z",
     "shell.execute_reply.started": "2024-01-16T21:38:52.792029Z"
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_subset = X[[\"lab4\", \"quiz1\"]]  # Let's consider a subset of the data for visualization\n",
    "X_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following model, this decision boundary is created by asking one question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:52.810830Z",
     "iopub.status.busy": "2024-01-16T21:38:52.809987Z",
     "iopub.status.idle": "2024-01-16T21:38:53.170852Z",
     "shell.execute_reply": "2024-01-16T21:38:53.169080Z",
     "shell.execute_reply.started": "2024-01-16T21:38:52.810802Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "depth = 1 #LINDA - this is a \"single question\", since depth = 1\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset.values, y)\n",
    "model.score(X_subset.values, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset.values, y)))\n",
    "plot_tree_decision_boundary_and_tree(model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following model, this decision boundary is created by asking two questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:53.173319Z",
     "iopub.status.busy": "2024-01-16T21:38:53.172283Z",
     "iopub.status.idle": "2024-01-16T21:38:53.561866Z",
     "shell.execute_reply": "2024-01-16T21:38:53.560934Z",
     "shell.execute_reply.started": "2024-01-16T21:38:53.173255Z"
    }
   },
   "outputs": [],
   "source": [
    "depth = 2\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset.values, y)\n",
    "model.score(X_subset.values, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset.values, y)))\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\", fontsize=14\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the decision boundary with depth = 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:53.565983Z",
     "iopub.status.busy": "2024-01-16T21:38:53.565283Z",
     "iopub.status.idle": "2024-01-16T21:38:54.274508Z",
     "shell.execute_reply": "2024-01-16T21:38:54.273433Z",
     "shell.execute_reply.started": "2024-01-16T21:38:53.565920Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "depth = 4\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset.values, y)\n",
    "model.score(X_subset.values, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset.values, y)))\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the decision boundary with depth = 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:54.275908Z",
     "iopub.status.busy": "2024-01-16T21:38:54.275542Z",
     "iopub.status.idle": "2024-01-16T21:38:55.112962Z",
     "shell.execute_reply": "2024-01-16T21:38:55.112078Z",
     "shell.execute_reply.started": "2024-01-16T21:38:54.275885Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "depth = 6\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset.values, y)\n",
    "model.score(X_subset.values, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset.values, y)))\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.114190Z",
     "iopub.status.busy": "2024-01-16T21:38:55.113850Z",
     "iopub.status.idle": "2024-01-16T21:38:55.359776Z",
     "shell.execute_reply": "2024-01-16T21:38:55.358598Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.114157Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = np.arange(1, 18)\n",
    "errors = []\n",
    "for max_depth in max_depths:\n",
    "    error = 1 - DecisionTreeClassifier(max_depth=max_depth).fit(X_subset, y).score(\n",
    "        X_subset, y\n",
    "    )\n",
    "    errors.append(error)\n",
    "plt.plot(max_depths, errors)\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Our model has 0% error for depths >= 6!! \n",
    "- But it's also becoming more and more specific and sensitive to the training data.  \n",
    "- Is it good or bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA \n",
    "\n",
    "    - at depth 6, all leaf cells are \"pure\" \n",
    "    \n",
    "    - ie. all leaves only have data in them that should be there\n",
    "    \n",
    "    - the error value is at 0!!\n",
    "\n",
    "    - this isn't perfect since we have overfit the data. New data might not fit well into our model since the data structure is so complex. This model won't generalize well\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Although the plot above (complexity hyperparameter vs error) is more popular, we could also look at the same plot flip the $y$-axis, i.e., consider accuracy instead of error. \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.362140Z",
     "iopub.status.busy": "2024-01-16T21:38:55.361503Z",
     "iopub.status.idle": "2024-01-16T21:38:55.566658Z",
     "shell.execute_reply": "2024-01-16T21:38:55.565698Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.362099Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = np.arange(1, 18)\n",
    "accuracies = []\n",
    "for max_depth in max_depths:\n",
    "    accuracy = (\n",
    "        DecisionTreeClassifier(max_depth=max_depth).fit(X_subset, y).score(X_subset, y)\n",
    "    )\n",
    "    accuracies.append(accuracy)\n",
    "plt.plot(max_depths, accuracies)\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ð¤ Eva's questions\n",
    "\n",
    "![](../img/eva-think.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this point Eva is wondering about the following questions. \n",
    "\n",
    "- How to pick the **best depth**? \n",
    "- How can we make sure that the model we have built **would do reasonably well on new data** in the wild when it's deployed? \n",
    "- Which of the following rules learned by the decision tree algorithm are likely to generalize better to new data? \n",
    "\n",
    "> Rule 1: If class_attendance == 1 then grade is A+. \n",
    "\n",
    "> Rule 2: If lab3 > 83.5 and quiz1 <= 83.5 and lab2 <= 88 then quiz2 grade is A+\n",
    "\n",
    "To better understand the material in the next sections, think about these questions on your own or discuss them with your friend/neighbour before proceeding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization: Fundamental goal of ML\n",
    "\n",
    "> **To generalize beyond what we see in the training examples**\n",
    "\n",
    "We only have access to limited amount of training data and we want to learn a mapping function which would predict targets reasonably well for examples beyond this training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Example: Imagine that a learner sees the following images and corresponding labels. \n",
    "\n",
    "<!-- ![](img/generalization-train.png) -->\n",
    "<!-- <center>\n",
    "<img src='img/generalization-train.png' width=\"600\" height=\"600\" />\n",
    "</center>     -->\n",
    "<img src='img/generalization-train.png' width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalizing to unseen data\n",
    "\n",
    "- Now the learner is presented with new images (1 to 4) for prediction. \n",
    "- What prediction would you expect for each image?   \n",
    "\n",
    "<!-- ![](img/generalization-predict.png) -->\n",
    "\n",
    "<!-- <center>\n",
    "<img src='img/generalization-predict.png' width=\"1000\" height=\"1000\" />\n",
    "</center>     -->\n",
    "\n",
    "<img src='img/generalization-predict.png' width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Goal: We want the learner to be able to generalize beyond what it has seen in the training data.\n",
    "- But these new examples should be representative of the training data. That is they should have the same characteristics as the training data. \n",
    "- In this example, we would like the leaner to be able to predict labels for test examples 1, 2, and 3 accurately. Although 2, 3 don't exactly occur in the training data, they are very much similar to the images in the training data. That said, is it fair to expect the learner to label image 4 correctly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. Generalization error \n",
    "\n",
    "- Given a model $M$, in ML, people usually talk about two kinds of errors of $M$. \n",
    "    1. Error on the training data: $error_{training}(M)$ \n",
    "    2. Error on the entire distribution $D$ of data: $error_{D}(M)$\n",
    "    \n",
    "- We are interested in the error on the entire distribution     \n",
    "    - ... But we do not have access to the entire distribution ð\n",
    "    - LINDA - since we don't have access to the entire distribution (ie. the data we use our model on (not the training data)), we want to try to estimate the generalization error in other ways (below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting [[video](https://youtu.be/h2AEobwcUQw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How to approximate generalization error? \n",
    "\n",
    "A common way is **data splitting**. \n",
    "- Keep aside some randomly selected portion from the training data.\n",
    "- `fit` (train) a model on the training portion only. \n",
    "- `score` (assess) the trained model on this set aside data to get a sense of how well the model would be able to generalize.\n",
    "- Pretend that the kept aside data is representative of the real distribution $D$ of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/eva-good-idea.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.568690Z",
     "iopub.status.busy": "2024-01-16T21:38:55.568048Z",
     "iopub.status.idle": "2024-01-16T21:38:55.578123Z",
     "shell.execute_reply": "2024-01-16T21:38:55.576161Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.568666Z"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# scikit-learn train_test_split\n",
    "url = \"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\"\n",
    "IFrame(src=url, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We can pass `X` and `y` or a dataframe with both `X` and `y` in it. \n",
    "- We can also specify the train or test split sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Simple train/test split \n",
    "\n",
    "- The picture shows an 80%-20% split of a toy dataset with 10 examples. \n",
    "- The data is shuffled before splitting. \n",
    "- Usually when we do machine learning we split the data before doing anything and put the test data in an imaginary chest lock. \n",
    "\n",
    "- LINDA - use 80% of the data you have for training the model, use 20% to check if the model works! (don't use the 20% for training)\n",
    "\n",
    "<!-- ![](img/train-test-split.png) -->\n",
    "\n",
    "<img src='img/train-test-split.png' width=\"900\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.580937Z",
     "iopub.status.busy": "2024-01-16T21:38:55.580120Z",
     "iopub.status.idle": "2024-01-16T21:38:55.591519Z",
     "shell.execute_reply": "2024-01-16T21:38:55.590187Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.580890Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's demonstrate this with the canada usa cities data\n",
    "# The data is available in the data directory\n",
    "df = pd.read_csv(\"data/canada_usa_cities.csv\")\n",
    "X = df.drop(columns=[\"country\"])\n",
    "y = df[\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.593261Z",
     "iopub.status.busy": "2024-01-16T21:38:55.592842Z",
     "iopub.status.idle": "2024-01-16T21:38:55.611558Z",
     "shell.execute_reply": "2024-01-16T21:38:55.609687Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.593216Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.613785Z",
     "iopub.status.busy": "2024-01-16T21:38:55.613309Z",
     "iopub.status.idle": "2024-01-16T21:38:55.629132Z",
     "shell.execute_reply": "2024-01-16T21:38:55.627716Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.613753Z"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.631450Z",
     "iopub.status.busy": "2024-01-16T21:38:55.630576Z",
     "iopub.status.idle": "2024-01-16T21:38:55.652207Z",
     "shell.execute_reply": "2024-01-16T21:38:55.650930Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.631408Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")  # 80%-20% train test split on X and y\n",
    "\n",
    "# Print shapes\n",
    "shape_dict = {\n",
    "    \"Data portion\": [\"X\", \"y\", \"X_train\", \"y_train\", \"X_test\", \"y_test\"],\n",
    "    \"Shape\": [\n",
    "        X.shape,\n",
    "        y.shape,\n",
    "        X_train.shape,\n",
    "        y_train.shape,\n",
    "        X_test.shape,\n",
    "        y_test.shape,\n",
    "    ],\n",
    "}\n",
    "\n",
    "shape_df = pd.DataFrame(shape_dict) #LINDA - we put the shape_dict into a df just for display, no other reason here\n",
    "shape_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA\n",
    "\n",
    "--  x_train, X_test, y_train, y_test = train_test_split ... is called \"unpacking a tuple\" in Python - can look this up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Creating `train_df` and `test_df`\n",
    "\n",
    "- Sometimes we want to keep the target in the train split for EDA or for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.656180Z",
     "iopub.status.busy": "2024-01-16T21:38:55.653978Z",
     "iopub.status.idle": "2024-01-16T21:38:55.687521Z",
     "shell.execute_reply": "2024-01-16T21:38:55.685751Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.656126Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=123\n",
    ")  # 80%-20% train test split on df\n",
    "X_train, y_train = train_df.drop(columns=[\"country\"]), train_df[\"country\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"country\"]), test_df[\"country\"]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.690321Z",
     "iopub.status.busy": "2024-01-16T21:38:55.689298Z",
     "iopub.status.idle": "2024-01-16T21:38:55.936262Z",
     "shell.execute_reply": "2024-01-16T21:38:55.934466Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.690275Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X.iloc[:, 0], X.iloc[:, 1], y, s=12)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:55.939492Z",
     "iopub.status.busy": "2024-01-16T21:38:55.938132Z",
     "iopub.status.idle": "2024-01-16T21:38:57.607921Z",
     "shell.execute_reply": "2024-01-16T21:38:57.606860Z",
     "shell.execute_reply.started": "2024-01-16T21:38:55.939403Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LINDA - if we don't specify a depth for the DecisionTreeClassifier,\n",
    "# the tree will go however deep is needed to be perfect\n",
    "\n",
    "model = DecisionTreeClassifier() \n",
    "model.fit(X_train.values, y_train.values)\n",
    "custom_plot_tree(model, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine the train and test accuracies with the split now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:57.610469Z",
     "iopub.status.busy": "2024-01-16T21:38:57.609766Z",
     "iopub.status.idle": "2024-01-16T21:38:57.619685Z",
     "shell.execute_reply": "2024-01-16T21:38:57.618517Z",
     "shell.execute_reply.started": "2024-01-16T21:38:57.610447Z"
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Train accuracy:   %0.3f\" % model.score(X_train.values, y_train.values))\n",
    "print(\"Test accuracy:   %0.3f\" % model.score(X_test.values, y_test.values))\n",
    "\n",
    "#train accurary will be 100% since we didn't set a depth for the classifier (the model is very complex)\n",
    "#test accurary will be lower, since the model is overfit to the training data (not general enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:57.621913Z",
     "iopub.status.busy": "2024-01-16T21:38:57.620891Z",
     "iopub.status.idle": "2024-01-16T21:38:59.409006Z",
     "shell.execute_reply": "2024-01-16T21:38:59.407622Z",
     "shell.execute_reply.started": "2024-01-16T21:38:57.621886Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_tree_decision_boundary_and_tree(model, X, y, height=10, width=18, eps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:59.411222Z",
     "iopub.status.busy": "2024-01-16T21:38:59.410145Z",
     "iopub.status.idle": "2024-01-16T21:38:59.746897Z",
     "shell.execute_reply": "2024-01-16T21:38:59.745171Z",
     "shell.execute_reply.started": "2024-01-16T21:38:59.411201Z"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6), subplot_kw={\"xticks\": (), \"yticks\": ()})\n",
    "plot_tree_decision_boundary(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eps=10,\n",
    "    x_label=\"longitude\",\n",
    "    y_label=\"latitude\",\n",
    "    ax=ax[0],\n",
    "    title=\"Decision tree model on the train data\",\n",
    ")\n",
    "plot_tree_decision_boundary(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    eps=10,\n",
    "    x_label=\"longitude\",\n",
    "    y_label=\"latitude\",\n",
    "    ax=ax[1],\n",
    "    title=\"Decision tree model on the test data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Useful arguments of `train_test_split`: \n",
    "    - `test_size`\n",
    "    - `train_size` \n",
    "    - `random_state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `test_size`, `train_size` arguments\n",
    "\n",
    "- Let's us specify how we want to split the data. \n",
    "- We can specify either of the two. See the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "- There is no hard and fast rule on what split sizes should we use. \n",
    "    - It depends upon how much data is available to you. \n",
    "- Some common splits are 90/10, 80/20, 70/30 (training/test).\n",
    "- In the above example, we used 80/20 split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `random_state` argument\n",
    "\n",
    "- The data is shuffled before splitting which is crucial step. (You will explore this in the lab.) \n",
    "- The `random_state` argument controls this shuffling. \n",
    "- In the example above we used `random_state=123`. If you run this notebook with the same `random_state` it should give you exactly the same split. \n",
    "    - Useful when you want reproducible results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/validation/test split\n",
    "\n",
    "- Some of you may have heard of \"**validation**\" data.\n",
    "- Sometimes it's a good idea to have a separate data for **hyperparameter tuning**.\n",
    "\n",
    "<!-- ![](img/train-valid-test-split.png) -->\n",
    "\n",
    "<img src='img/train-valid-test-split.png' width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We will try to use \"validation\" (or \"development\") to refer to data where we have access to the target values.\n",
    "  - But, unlike the training data,\n",
    "    - we only use this for hyperparameter tuning and model assessment; \n",
    "    - we don't pass these into `fit`.\n",
    "- We will try to use \"test\" to refer to data where we have access to the target values \n",
    "  - But, unlike training and validation data,\n",
    "    - we neither use it in training nor hyperparameter optimization. \n",
    "  - We only use it **once** to evaluate the performance of the best performing model on the validation set.   \n",
    "  - We lock it in a \"vault\" until we're ready to evaluate. \n",
    "\n",
    "LINDA -\"deployement\" split refers to the real world data you will be using your model on\n",
    "\n",
    "\n",
    "Note that there isn't good concensus on the terminology of what is validation and what is test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "___\n",
    "Validation data is also referred to as **development data** or **dev set** for short.  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Deployment\" data\n",
    "\n",
    "- After we build and finalize a model, we deploy it, and then the model deals with the data in the wild. \n",
    "- We will use \"deployment\" to refer to this data, where we do **not** have access to the target values.\n",
    "- Deployment error is what we _really_ care about.\n",
    "- We use validation and test errors as proxies for deployment error, and we hope they are similar.\n",
    "- So, if our model does well on the validation and test data, we hope it will do well on deployment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary of train, validation, test, and deployment data\n",
    "\n",
    "|         | `fit` | `score` | `predict` |\n",
    "|----------|-------|---------|-----------|\n",
    "| Train    | âï¸      | âï¸      | âï¸         |\n",
    "| Validation |      | âï¸      | âï¸         |\n",
    "| Test    |       |  once   | once         |\n",
    "| Deployment    |       |       | âï¸         | \n",
    "\n",
    "\n",
    "You can typically expect $E_{train} < E_{validation} < E_{test} < E_{deployment}$.\n",
    "\n",
    "\n",
    "LINDA -deployement is the real world data we use our model on\n",
    "- if our model works well on the test split, then we will deploy and use our model on real world data\n",
    "- E_{validation} is better than E_{test} because you can do validation over and over again, but we only use test data once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ââ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iClicker Exercise 3.1 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/FSLV**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) A decision tree model with no depth (the default `max_depth` in `sklearn`) is likely to perform very well on the deployment data. \n",
    "- (B) Data splitting helps us assess how well our model would generalize. \n",
    "- (C) Deployment data is only scored once.  \n",
    "- (D) Validation data could be used for hyperparameter optimization. \n",
    "- (E) It's recommended that data be shuffled before splitting it into `train` and `test` sets..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA - Answers to clicker:\n",
    "\n",
    "a) false\n",
    "\n",
    "b) TRUE\n",
    "\n",
    "c) false\n",
    "\n",
    "d) TRUE\n",
    "\n",
    "e) TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Exercise 3.a: Questions for discussion\n",
    "1. Why you can typically expect $E_{train}\\  < E_{validation}\\  < E_{test}\\  < E_{deployment}\\ $.\n",
    "2. Discuss the consequences of not shuffling before splitting the data in `train_test_split`. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](../img/eva-coffee.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation [[video](https://youtu.be/4cv8VYonepA)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems with single train/validation split\n",
    "\n",
    "- Only using a portion of your data for training and only a portion for validation.\n",
    "- If your dataset is small you might end up with a tiny training and/or validation set.\n",
    "- You might be unlucky with your splits such that they don't align well or don't well represent your test data.\n",
    "\n",
    "<!-- ![](img/train-valid-test-split.png) -->\n",
    "\n",
    "<!-- <img src='img/train-valid-test-split.png' width=\"1500\" height=\"1500\" /> -->\n",
    "<img src='img/train-valid-test-split.png' width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation to the rescue!! \n",
    "\n",
    "- Cross-validation provides a solution to this problem. \n",
    "- Split the data into $k$ folds ($k>2$, often $k=10$). In the picture below $k=4$.\n",
    "- Each \"fold\" gets a turn at being the validation set.\n",
    "- Note that cross-validation doesn't shuffle the data; it's done in `train_test_split`.\n",
    "\n",
    "LINDA \n",
    "- basically, repeat the validation step using different splits over and over, to ensure the entire training set is actually used to train the model (ei. if you only did it once, the \"validation\" set would be left out of the training data)\n",
    "- don't reshuffle the indices when repeating validation since we want to cover rows we haven't yet trained out model on\n",
    "\n",
    "\n",
    "<!-- ![](img/cross-validation.png) -->\n",
    "<img src='img/cross-validation.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each fold gives a score and we usually average our $k$ results. \n",
    "- It's better to examine the variation in the scores across folds.  \n",
    "- Gives a more **robust** measure of error on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation using `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:59.751839Z",
     "iopub.status.busy": "2024-01-16T21:38:59.751210Z",
     "iopub.status.idle": "2024-01-16T21:38:59.757610Z",
     "shell.execute_reply": "2024-01-16T21:38:59.755544Z",
     "shell.execute_reply.started": "2024-01-16T21:38:59.751804Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:59.759685Z",
     "iopub.status.busy": "2024-01-16T21:38:59.759052Z",
     "iopub.status.idle": "2024-01-16T21:38:59.822315Z",
     "shell.execute_reply": "2024-01-16T21:38:59.820172Z",
     "shell.execute_reply.started": "2024-01-16T21:38:59.759631Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linda \n",
    "- cv=10 means there are 10 folds\n",
    "- cross_val_score gets the average cross-validation score from all of the different folds used for validation to give us a more accurate score for our multi-validated model (give's us the .score for the yellow parts)\n",
    "- we want a low standard deviation between the validations (high standard deviation means there's a lot of noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:59.824788Z",
     "iopub.status.busy": "2024-01-16T21:38:59.823821Z",
     "iopub.status.idle": "2024-01-16T21:38:59.831154Z",
     "shell.execute_reply": "2024-01-16T21:38:59.829624Z",
     "shell.execute_reply.started": "2024-01-16T21:38:59.824763Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Average cross-validation score = {np.mean(cv_scores):.2f}\")\n",
    "print(f\"Standard deviation of cross-validation score = {np.std(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Under the hood\n",
    "\n",
    "- It creates `cv` folds on the data.\n",
    "- In each fold, it fits the model on the training portion and scores on the validation portion. \n",
    "- The **output is a list of validation scores** in each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using `cross_validate`\n",
    "\n",
    "- Similar to `cross_val_score` but more powerful.\n",
    "- Let's us **access training and validation scores**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:59.833333Z",
     "iopub.status.busy": "2024-01-16T21:38:59.833040Z",
     "iopub.status.idle": "2024-01-16T21:38:59.906125Z",
     "shell.execute_reply": "2024-01-16T21:38:59.905206Z",
     "shell.execute_reply.started": "2024-01-16T21:38:59.833311Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(\n",
    "    model, X_train, y_train, cv=10, return_train_score=True)\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:59.907725Z",
     "iopub.status.busy": "2024-01-16T21:38:59.907144Z",
     "iopub.status.idle": "2024-01-16T21:38:59.917524Z",
     "shell.execute_reply": "2024-01-16T21:38:59.916710Z",
     "shell.execute_reply.started": "2024-01-16T21:38:59.907699Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.DataFrame(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA\n",
    "- we aren't concerned with efficiency in this course, so ignore fit_time and score_time\n",
    "- \"test_score\" here is what we've called validation_score in the notes\n",
    "- use cross-validation to judge how good our model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "___\n",
    "Keep in mind that cross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to **evaluate** how well the model will generalize to unseen data. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "___\n",
    "Note that both `cross_val_score` and `cross_validate` functions do not shuffle the data. Check out [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold), where proportions of classes is the same in each fold as they are in the whole dataset. By default, `sklearn` uses `StratifiedKFold` when carrying out cross-validation for classification problems. \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:38:59.918677Z",
     "iopub.status.busy": "2024-01-16T21:38:59.918462Z",
     "iopub.status.idle": "2024-01-16T21:39:00.136292Z",
     "shell.execute_reply": "2024-01-16T21:39:00.135251Z",
     "shell.execute_reply.started": "2024-01-16T21:38:59.918658Z"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mglearn_utils\n",
    "mglearn_utils.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA - again, here \"test_data\" is what we refer to as \"validation_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our **typical supervised learning set up** is as follows: \n",
    "\n",
    "- We are **given** training data with features `X` and target `y`\n",
    "- We **split** the data into train and test portions: `X_train, y_train, X_test, y_test`\n",
    "- We carry out **hyperparameter optimization** using cross-validation on the train portion: `X_train` and `y_train`. \n",
    "- We **assess our best performing model** on the test portion: `X_test` and `y_test`.  \n",
    "- What we care about is the ***test error***, which tells us how well our model can be **generalized**.\n",
    "- If this **test error is \"reasonable\" we deploy** the model which will be used on new unseen examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA - max_depth for the Decision Tree Classifier is the first hyperparameter we have encountered in the course\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.138095Z",
     "iopub.status.busy": "2024-01-16T21:39:00.137855Z",
     "iopub.status.idle": "2024-01-16T21:39:00.208411Z",
     "shell.execute_reply": "2024-01-16T21:39:00.207164Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.138074Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "model = DecisionTreeClassifier(max_depth=10)\n",
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the *unfortunate naming* above: `test_score`, which should have been *validation score*. We usually leave this as is, but if you insist to be consistent, you could rename by, for example, replacing the key `test_score` with `val_score` in the `scores` dictionary, or by renaming the dataframe columns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.209586Z",
     "iopub.status.busy": "2024-01-16T21:39:00.209353Z",
     "iopub.status.idle": "2024-01-16T21:39:00.227019Z",
     "shell.execute_reply": "2024-01-16T21:39:00.225012Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.209565Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scores).rename({'test_score': 'val_score'}, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But given that we need to use `cross_validate` function over and over in different places, it's better to just keep in mind this naming problem, and decide to rename on a case by case basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.230926Z",
     "iopub.status.busy": "2024-01-16T21:39:00.229633Z",
     "iopub.status.idle": "2024-01-16T21:39:00.236703Z",
     "shell.execute_reply": "2024-01-16T21:39:00.235648Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.230874Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "    \"\"\"\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.239097Z",
     "iopub.status.busy": "2024-01-16T21:39:00.237954Z",
     "iopub.status.idle": "2024-01-16T21:39:00.283907Z",
     "shell.execute_reply": "2024-01-16T21:39:00.282566Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.239069Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"Decision tree\"] = mean_std_cross_val_scores(\n",
    "    model, X_train, y_train, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA - The above 2 equations will be used throughout the course to nicely display validation scores and standard deviations. Be familiar with it!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do we know whether this test score (validation score) is reasonable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting, overfitting, the fundamental trade-off, the golden rule [[video](https://youtu.be/Ihay8yE5KTI)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of errors\n",
    "\n",
    "Imagine that your train and validation errors do not align with each other. How do you diagnose the problem?  \n",
    "\n",
    "We're going to think about 4 types of errors:\n",
    "\n",
    "- $E_\\textrm{train}\\ $ is your training error (or mean train error from cross-validation).\n",
    "- $E_\\textrm{valid}\\ $  is your validation error (or mean validation error from cross-validation).\n",
    "- $E_\\textrm{test}\\ $  is your test error.\n",
    "- $E_\\textrm{best}\\ $  is the best possible error you could get for a given problem (often unknown, but desired)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.286345Z",
     "iopub.status.busy": "2024-01-16T21:39:00.285646Z",
     "iopub.status.idle": "2024-01-16T21:39:00.344598Z",
     "shell.execute_reply": "2024-01-16T21:39:00.343164Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.286287Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1)  # decision stump\n",
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "print(\"Train error:   %0.3f\" % (1 - np.mean(scores[\"train_score\"])))\n",
    "print(\"Validation error:   %0.3f\" % (1 - np.mean(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If your **model is too simple**, like `DummyClassifier` or `DecisionTreeClassifier` with `max_depth=1`, it's not going to pick up on some random quirks in the data but it won't even capture useful patterns in the training data.\n",
    "- The model won't be very good in general. ***Both train and validation errors would be high***. This is **underfitting**.\n",
    "- The gap between train and validation error is going to be lower.\n",
    "- $E_\\textrm{best} \\lt E_\\textrm{train} \\lesssim E_\\textrm{valid}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINDA - E_best is the theoretical best we could get, NOT what we actually get in real life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.347480Z",
     "iopub.status.busy": "2024-01-16T21:39:00.346659Z",
     "iopub.status.idle": "2024-01-16T21:39:00.426715Z",
     "shell.execute_reply": "2024-01-16T21:39:00.425406Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.347433Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=None)\n",
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "print(\"Train error:   %0.3f\" % (1 - np.mean(scores[\"train_score\"])))\n",
    "print(\"Validation error:   %0.3f\" % (1 - np.mean(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If your **model is very complex**, like a `DecisionTreeClassifier(max_depth=None)`, then you will learn unreliable patterns in order to get every single training example correct.\n",
    "- The training error is going to be very low but there will be a ***big gap between the training error and the validation error***. This is **overfitting**.\n",
    "- In overfitting scenario, usually we'll see: \n",
    "$E_\\textrm{train} \\lt E_\\textrm{best}  \\lt E_\\textrm{valid}$\n",
    "- In general, if $E_\\textrm{train}\\ $ is low, we are likely to be in the overfitting scenario. It is fairly common to have at least a bit of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So the validation error does not necessarily decrease with the training error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.427796Z",
     "iopub.status.busy": "2024-01-16T21:39:00.427587Z",
     "iopub.status.idle": "2024-01-16T21:39:00.974846Z",
     "shell.execute_reply": "2024-01-16T21:39:00.973885Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.427777Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "results_dict = {\n",
    "    \"depth\": [],\n",
    "    \"mean_train_error\": [],\n",
    "    \"mean_cv_error\": [],\n",
    "    \"std_cv_error\": [],\n",
    "    \"std_train_error\": [],\n",
    "}\n",
    "param_grid = {\"max_depth\": np.arange(1, 16)}\n",
    "\n",
    "for depth in param_grid[\"max_depth\"]:\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "    results_dict[\"depth\"].append(depth)\n",
    "    results_dict[\"mean_cv_error\"].append(1 - np.mean(scores[\"test_score\"]))\n",
    "    results_dict[\"mean_train_error\"].append(1 - np.mean(scores[\"train_score\"]))\n",
    "    results_dict[\"std_cv_error\"].append(scores[\"test_score\"].std())\n",
    "    results_dict[\"std_train_error\"].append(scores[\"train_score\"].std())\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df = results_df.set_index(\"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.976114Z",
     "iopub.status.busy": "2024-01-16T21:39:00.975858Z",
     "iopub.status.idle": "2024-01-16T21:39:00.989024Z",
     "shell.execute_reply": "2024-01-16T21:39:00.987688Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.976096Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:00.991389Z",
     "iopub.status.busy": "2024-01-16T21:39:00.990630Z",
     "iopub.status.idle": "2024-01-16T21:39:01.277303Z",
     "shell.execute_reply": "2024-01-16T21:39:01.275974Z",
     "shell.execute_reply.started": "2024-01-16T21:39:00.991342Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results_df[[\"mean_train_error\", \"mean_cv_error\"]].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Here, for larger depths we observe that the training error is close to 0 but validation error goes up and down. \n",
    "- As we make **more complex models we start encoding random quirks** in the data, which are not grounded in reality.  \n",
    "- These **random quirks do not generalize** well to new data. \n",
    "- This problem of failing to be able to generalize to the validation data or test data is called **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The \"fundamental tradeoff\" of supervised learning:\n",
    "\n",
    "\n",
    "> **As you increase model complexity, $E_\\textrm{train}\\ $ tends to go down but $E_\\textrm{valid}-E_\\textrm{train}\\ $ tends to go up.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias vs variance tradeoff \n",
    "\n",
    "- The fundamental trade-off is also called the bias/variance tradeoff in supervised machine learning.\n",
    "\n",
    "**Bias**\n",
    ": the tendency to consistently learn the same wrong thing (high **bias corresponds to underfitting**)\n",
    "\n",
    "**Variance** \n",
    ": the tendency to learn random things irrespective of the real signal (high **variance corresponds to overfitting**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "___\n",
    "Check out [this article by Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) for some approachable explanation on machine learning fundamentals and bias-variance tradeoff. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to pick a model that would generalize better?\n",
    "\n",
    "- We want to avoid both underfitting and overfitting. \n",
    "- We want to be consistent with the training data but we don't want to rely too much on it. \n",
    "\n",
    "<img src='img/malp_0201.png' width=\"600\" />\n",
    "\n",
    "<!-- ![](img/malp_0201.png) -->\n",
    "\n",
    "[source](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#relation-of-model-complexity-to-dataset-size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:01.279490Z",
     "iopub.status.busy": "2024-01-16T21:39:01.278723Z",
     "iopub.status.idle": "2024-01-16T21:39:01.293086Z",
     "shell.execute_reply": "2024-01-16T21:39:01.291654Z",
     "shell.execute_reply.started": "2024-01-16T21:39:01.279463Z"
    },
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:01.295170Z",
     "iopub.status.busy": "2024-01-16T21:39:01.294752Z",
     "iopub.status.idle": "2024-01-16T21:39:01.304377Z",
     "shell.execute_reply": "2024-01-16T21:39:01.302131Z",
     "shell.execute_reply.started": "2024-01-16T21:39:01.295147Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validate_std(*args, **kwargs):\n",
    "    \"\"\"Like cross_validate, except also gives the standard deviation of the score\"\"\"\n",
    "    res = pd.DataFrame(cross_validate(*args, **kwargs))\n",
    "    res_mean = res.mean()\n",
    "    res_mean[\"std_test_score\"] = res[\"test_score\"].std()\n",
    "    if \"train_score\" in res:\n",
    "        res_mean[\"std_train_score\"] = res[\"train_score\"].std()\n",
    "    return res_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function makes it more convenient to produce the same results that we already had above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:01.306615Z",
     "iopub.status.busy": "2024-01-16T21:39:01.306155Z",
     "iopub.status.idle": "2024-01-16T21:39:01.383667Z",
     "shell.execute_reply": "2024-01-16T21:39:01.382355Z",
     "shell.execute_reply.started": "2024-01-16T21:39:01.306589Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    cross_validate_std(model, X_train, y_train, cv=10, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### test score vs. cross-validation score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There are many subtleties here, and there is no perfect answer, but a  **common practice is to pick the model with minimum cross-validation error**.\n",
    "\n",
    "\n",
    "Recall from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:01.385257Z",
     "iopub.status.busy": "2024-01-16T21:39:01.384853Z",
     "iopub.status.idle": "2024-01-16T21:39:01.397213Z",
     "shell.execute_reply": "2024-01-16T21:39:01.395828Z",
     "shell.execute_reply.started": "2024-01-16T21:39:01.385236Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:01.399431Z",
     "iopub.status.busy": "2024-01-16T21:39:01.398973Z",
     "iopub.status.idle": "2024-01-16T21:39:01.408065Z",
     "shell.execute_reply": "2024-01-16T21:39:01.406505Z",
     "shell.execute_reply.started": "2024-01-16T21:39:01.399376Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "best_depth_error = np.min(results_df[\"mean_cv_error\"])\n",
    "best_depth_index = np.argmin(results_df[\"mean_cv_error\"])\n",
    "best_depth = results_df.index.values[best_depth_index]\n",
    "\n",
    "print(\"The minimum validation error is %0.3f at max_depth = %d \" % \n",
    "      (best_depth_error, best_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's make a decision tree model with `max_depth`= 5 and try this model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T21:39:01.409656Z",
     "iopub.status.busy": "2024-01-16T21:39:01.409362Z",
     "iopub.status.idle": "2024-01-16T21:39:01.426146Z",
     "shell.execute_reply": "2024-01-16T21:39:01.424635Z",
     "shell.execute_reply.started": "2024-01-16T21:39:01.409627Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=best_depth)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Error on test set is %0.3f \" % (1 - model.score(X_test, y_test)))\n",
    "print(\"The minimum validation error is %0.3f \" % best_depth_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The test error is comparable with the cross-validation error. \n",
    "- Do we feel confident that this model would give similar performace when deployed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The golden rule <a name=\"4\"></a>\n",
    "\n",
    "- Even though we care the most about test error **THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY**. \n",
    "- We have to be very careful not to violate it while developing our ML pipeline. \n",
    "- Even experts end up breaking it sometimes which leads to misleading results and lack of generalization on the real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Golden rule violation: Example 1  \n",
    "\n",
    "<!-- ![](img/golden_rule_violation.png) -->\n",
    "\n",
    "<img src='img/golden_rule_violation.png' width=\"500\" />\n",
    " \n",
    "<blockquote>\n",
    "   ... He attempted to reproduce the research, and found a major flaw: there was some overlap in the data used to both train and test the model. \n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Golden rule violation: Example 2  \n",
    "\n",
    "<img src='img/golden_rule_violation_2.png' width=\"500\" />\n",
    " \n",
    "<!-- ![](img/golden_rule_violation_2.png) -->\n",
    "\n",
    "<blockquote>\n",
    "  ... The Challenge rules state that you must only test your code twice a week, because thereâs an element of chance to the results. Baidu has admitted that it used multiple email accounts to test its code roughly 200 times in just under six months â over four times what the rules allow.\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden rule violation: Example 3\n",
    "\n",
    "![](../img/GPT-ToM.png)\n",
    "\n",
    "\n",
    "<blockquote>\n",
    "A few months ago, debates around whether contemporary large language models (LLMs) are showing theory of mind capabilities or not sparked the media...\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "...We show that NONE of the existing LLMs show signs of coherent ToM capabilities, including GPT-4.\n",
    "</blockquote>\n",
    "\n",
    "___\n",
    "\n",
    "***What happened?*** Common ToM test questions are on the internet, and were in the training set of the LLMs, generating hilarious cases of over-fitting.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we avoid violating golden rule? \n",
    "\n",
    "- Recall that when we split data, we put our test set in an imaginary vault.\n",
    "\n",
    "<img src='img/train-test-split.png' width=\"900\" />\n",
    "\n",
    "<!-- ![](img/train-test-split.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Here is the workflow we'll generally follow. \n",
    "\n",
    "- **Splitting**: Before doing anything, split the data `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` or `train_df` and `test_df` using `train_test_split`. \n",
    "- **Select the best model using cross-validation**: Use `cross_validate` with `return_train_score = True` so that we can get access to training scores in each fold. (If we want to plot train vs validation error plots, for instance.) \n",
    "- **Scoring on test data**: Finally score on the test data with the chosen hyperparameters to examine the generalization performance.\n",
    "\n",
    "**Again, there are many subtleties here we'll discuss the golden rule multiple times throughout the course and in the program.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## ââ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iClicker Exercise 3.2 \n",
    "\n",
    "**iClicker cloud join link: https://join.iclicker.com/**\n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) $k$-fold cross-validation calls fit $k$ times.\n",
    "- (B) We use cross-validation to get a more robust estimate of model performance.\n",
    "- (C) If the mean train accuracy is much higher than the mean cross-validation accuracy it's likely to be a case of overfitting.\n",
    "- (D) The fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n",
    "- (E) A decision stump on a complicated classification problem is likely to underfit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 3.b\n",
    "\n",
    "Underfitting or overfitting? \n",
    "1. If the mean train accuracy is much higher than the mean cross-validation accuracy.\n",
    "2. If the mean train accuracy and the mean cross-validation accuracy are both low and relatively similar in value.\n",
    "3. Decision tree with no limit on the depth. \n",
    "4. Decision stump on a complicated classification problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " Exercise 3.b: Solution\n",
    "1. Overfitting\n",
    "2. Underfitting\n",
    "3. Overfitting\n",
    "4. Underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 3.c\n",
    " \n",
    "State whether True/False. \n",
    "\n",
    "1. In supervised learning, the training error is always lower than the validation error.\n",
    "2. The fundamental tradeoff of ML states that as training error goes down, the gap between the validation error and training error goes up.\n",
    "3. More \"complicated\" models are more likely to overfit than \"simple\" ones.\n",
    "4. If we had an infinite amount of training data, overfitting would not be a problem.\n",
    "5. If our training error is extremely low, we are likely to be overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " Exercise 3.c: Solution\n",
    "1. False\n",
    "2. True\n",
    "3. True\n",
    "4. True\n",
    "5. True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- Importance of generalization in supervised machine learning \n",
    "- Data splitting as a way to approximate generalization error \n",
    "- Train, test, validation, deployment data\n",
    "- Cross-validation\n",
    "- A typical sequence of steps to train supervised machine learning models\n",
    "    - training the model on the train split\n",
    "    - tuning hyperparamters using the validation split\n",
    "    - checking the generalization performance on the test split \n",
    "- Overfitting, underfitting, the fundamental tradeoff, and the golden rule.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Write your reflections (takeaways, struggle points, and general comments) on this material in [our reflection Google Document](https://docs.google.com/document/d/1LWiR7dzzNNwOJR72LWgC25priLXGdyxEq9FWBia3wmw/edit?usp=sharing) so that I'll try to address those points in the next lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming up ...  \n",
    "\n",
    "- KNNs, SVM RBFs \n",
    "- Preprocessing\n",
    "    - Imputation\n",
    "    - Scaling\n",
    "    - One-hot encoding\n",
    "    - `sklearn` pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-seeyou.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
